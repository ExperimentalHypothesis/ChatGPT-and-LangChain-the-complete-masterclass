{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# We want to build a chat style model, not completion style model. \n",
    "### In previous we have just completion style model where input -> model -> output\n",
    "### Chat style model is different, because we have to discriminate between user message and chat message, there is also a system message that can specify how the model will behave\n",
    "\n",
    "![title](pic/Screenshot_20240518_193150.png)\n",
    "\n",
    "![title](pic/Screenshot_20240518_193621.png)\n",
    "\n",
    "### ChatGPT does not store history, so we always have to include the previous history. \n",
    "### For that we need to use ChatPromptTemplate, not just PromptTemplate. Here is diagram how it works behind the scene\n",
    "\n",
    "![title](pic/Screenshot_20240518_194410.png)\n"
   ],
   "id": "46118927be485573"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Example with no memory involved - se the answer that are completely out of reality",
   "id": "97a1221936ff7439"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T18:29:43.002140Z",
     "start_time": "2024-05-18T18:29:39.105556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# a bit CONFUSING here - as this should hold the whole history\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages=[HumanMessagePromptTemplate.from_template(\"{content}\")]\n",
    ")\n",
    "\n",
    "chain  = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "res = None\n",
    "while True:\n",
    "    content = input(\">> \")\n",
    "    res = chain({\"content\": content})    \n",
    "    print(res)\n",
    "    \n",
    "    if content == 'q':\n",
    "        break"
   ],
   "id": "7638b1912fa14d9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'q', 'text': 'Hello! How can I assist you today?'}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Memory = class that we can put inside a chain to keep track of history message. It stores data inside the chain. It is used twice 1]when we send question to the model, and 2] when we receive an answer\n",
    " \n",
    "![title](pic/Screenshot_20240518_200338.png)"
   ],
   "id": "c79726d959f08e3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# So when we first call our chain this is what happens. The memory inside the model stores our message + some additional field ![title](pic/Screenshot_20240518_201145.png)\n",
    "\n",
    "# When outputing the response it stores it also\n",
    "![title](pic/Screenshot_20240518_201145.png) "
   ],
   "id": "7bc33dbd967e47ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# There are many different kind of memory but mostly for completion models. We want chat based model so ConversationBufferMemory is used.",
   "id": "a13dd4a1b1a01f59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# So this is what is happening when we get response \n",
    "![title](pic/Screenshot_20240518_202302.png) \n",
    "# And this is when we continue replying\n",
    "![title](pic/Screenshot_20240518_202525.png) \n",
    "# The last step is to send the history list to ChatGPT\n",
    "![title](pic/Screenshot_20240518_202833.png) \n",
    " "
   ],
   "id": "ff4b394036dd241f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T18:37:51.144026Z",
     "start_time": "2024-05-18T18:37:33.720748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"messages\", # this is the key name that will be created inside the memory buffer\n",
    "    return_messages=True  # dont just throw strings but wrap them into HumanMessage... for chat like models\n",
    ")\n",
    "\n",
    "# a bit CONFUSING here - as this should hold the whole history\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\", \"messages\"],\n",
    "    messages=[\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # this needs to match with the memory_key inside the ConversationBufferMemory\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain  = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "res = None\n",
    "while True:\n",
    "    content = input(\">> \")\n",
    "    res = chain({\"content\": content})    \n",
    "    print(res)\n",
    "    \n",
    "    if content == 'q':\n",
    "        break"
   ],
   "id": "36cc3086238a22de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'what is 1+1?', 'messages': [HumanMessage(content='what is 1+1?'), AIMessage(content='1+1 equals 2.')], 'text': '1+1 equals 2.'}\n",
      "{'content': 'and 3 more?', 'messages': [HumanMessage(content='what is 1+1?'), AIMessage(content='1+1 equals 2.'), HumanMessage(content='and 3 more?'), AIMessage(content='1+1+1 equals 3.')], 'text': '1+1+1 equals 3.'}\n",
      "{'content': 'q', 'messages': [HumanMessage(content='what is 1+1?'), AIMessage(content='1+1 equals 2.'), HumanMessage(content='and 3 more?'), AIMessage(content='1+1+1 equals 3.'), HumanMessage(content='q'), AIMessage(content='Is there anything else I can assist you with?')], 'text': 'Is there anything else I can assist you with?'}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef767b02cdc4b2b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
